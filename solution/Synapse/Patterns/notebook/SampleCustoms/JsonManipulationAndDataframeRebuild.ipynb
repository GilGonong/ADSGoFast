{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\r\n",
        "from notebookutils import mssparkutils\r\n",
        "import pandas as pd\r\n",
        "Source = 'abfss://datalakelanding@arkstgdlsadsaqyeadsl.dfs.core.windows.net/accounts-response.json'\r\n",
        "sourceDF = spark.read.option(\"multiline\",\"true\").json(Source)\r\n",
        "sourceDF.show()\r\n",
        "#used for outputting to json in next cell block\r\n",
        "Target = 'abfss://datalakelanding@arkstgdlsadsaqyeadsl.dfs.core.windows.net/test-response.json'\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": 5,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-06-30T23:52:26.5033621Z",
              "session_start_time": "2022-06-30T23:52:26.5440818Z",
              "execution_start_time": "2022-06-30T23:53:09.518635Z",
              "execution_finish_time": "2022-06-30T23:53:26.2697106Z"
            },
            "text/plain": "StatementMeta(arkstgsynspads, 5, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+\n|            elements|          paging|\n+--------------------+----------------+\n|[[[[1606804958000...|[1000, [], 0, 8]|\n+--------------------+----------------+\n\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will take a dictionary and build it out completely for columns in a dataframe (so a nested dictionary / json )\r\n",
        "def as_row(obj):\r\n",
        "    if isinstance(obj, dict):\r\n",
        "        dictionary = {k: as_row(v) for k, v in obj.items()}\r\n",
        "        return Row(**dictionary)\r\n",
        "    elif isinstance(obj, list):\r\n",
        "        return [as_row(v) for v in obj]\r\n",
        "    else:\r\n",
        "        return obj\r\n",
        "\r\n",
        "#collect each array object of 'elements' and convert it to a list of dictionaries\r\n",
        "ss=sourceDF.select(\"elements\").collect()\r\n",
        "output=[i[0] for i in ss]\r\n",
        "fullList = []\r\n",
        "for obj in output:\r\n",
        "    for row in obj:\r\n",
        "        fullList.append(row.asDict(True))\r\n",
        "\r\n",
        "#we now want to iterate this list of dictionaries (or list of jsons) for manipulation and re-building into an amended list\r\n",
        "amendedList = []\r\n",
        "for obj in fullList:\r\n",
        "    obj[\"id\"] = str(obj[\"id\"])\r\n",
        "    #uncomment next two lines to use the function instead of other way\r\n",
        "    row = as_row(obj)\r\n",
        "    amendedList.append(row)\r\n",
        "    #amendedList.append(obj)\r\n",
        "\r\n",
        "#for testing\r\n",
        "#for x in amendedList:\r\n",
        "#    print(type(x))\r\n",
        "\r\n",
        "\r\n",
        "#re-build - note: pyspark seems a bit funky atm with the first run using pandas, will throw a warning and hide output, but normally still executes full code block (check for successful run)\r\n",
        "pdDF = pd.DataFrame([str(e) for e in amendedList])\r\n",
        "columns = [\"elements\"]\r\n",
        "outDF = spark.createDataFrame(pdDF, columns)\r\n",
        "outDF.show()\r\n",
        "\r\n",
        "#this was for testing writing to json \r\n",
        "newDF.write.format(\"json\").mode(\"overwrite\").save(Target)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": 5,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-06-30T23:53:30.1543634Z",
              "session_start_time": null,
              "execution_start_time": "2022-06-30T23:53:30.3152446Z",
              "execution_finish_time": "2022-06-30T23:53:33.1059755Z"
            },
            "text/plain": "StatementMeta(arkstgsynspads, 5, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  'JavaPackage' object is not callable\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n|            elements|\n+--------------------+\n|{'changeAuditStam...|\n|{'changeAuditStam...|\n|{'changeAuditStam...|\n|{'changeAuditStam...|\n|{'changeAuditStam...|\n|{'changeAuditStam...|\n|{'changeAuditStam...|\n|{'changeAuditStam...|\n+--------------------+\n\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#output = pd.DataFrame()\r\n",
        "#for x in amendedList:\r\n",
        "#    output = output.append(x, ignore_index=True)\r\n",
        "#sparkDF=spark.createDataFrame(output)\r\n",
        "#sparkDF.show() \r\n",
        "#for a in amendedList:\r\n",
        "  #  print(e)\r\n",
        "\r\n",
        "\r\n",
        "#df = pd.DataFrame(amendedList, columns=['elements'])\r\n",
        "#df = spark.createDataFrame(df)\r\n",
        "#df.show()\r\n"
      ],
      "outputs": [],
      "execution_count": 99,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\r\n",
        "{\r\n",
        "    \"sink\": {\r\n",
        "        \"storeSettings\": {\r\n",
        "            \"type\": \"AzureBlobFSWriteSettings\"\r\n",
        "        },\r\n",
        "        \"type\": \"ParquetSink\"\r\n",
        "    },\r\n",
        "    \"mappings\": [\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"LAST_NAME\",\r\n",
        "                \"type\": \"String\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"LASTNAME\",\r\n",
        "                \"type\": \"UTF8\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            }\r\n",
        "        },\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"UNIQUE_EMAIL\",\r\n",
        "                \"type\": \"String\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"UNIQUEEMAIL\",\r\n",
        "                \"type\": \"UTF8\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            }\r\n",
        "        },\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"FIRST_NAME\",\r\n",
        "                \"type\": \"String\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"FIRSTNAME\",\r\n",
        "                \"type\": \"UTF8\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            }\r\n",
        "        },\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"PERSON_ID\",\r\n",
        "                \"type\": \"Int64\",\r\n",
        "                \"physicalType\": \"NUMBER\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"PERSONID\",\r\n",
        "                \"type\": \"Int64\",\r\n",
        "                \"physicalType\": \"NUMBER\"\r\n",
        "            }\r\n",
        "        }\r\n",
        "    ]\r\n",
        "    \r\n",
        "}\r\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}