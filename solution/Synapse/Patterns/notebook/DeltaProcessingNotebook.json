{
	"name": "DeltaProcessingNotebook",
	"properties": {
		"folder": {
			"name": "FrameworkNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"metadata": {
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"TaskObject = \"\""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import random\n",
					"import json\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.functions import *\n",
					"\n",
					"session_id = random.randint(0,1000000)\n",
					"\n",
					"TaskObjectJson = json.loads(TaskObject)\n",
					"Source = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
					"Schema = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
					"Target = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
					"\n",
					"\n",
					"Source = Source + TaskObjectJson['Source']['Instance']['SourceRelativePath'] + \"/\" + TaskObjectJson['Source']['DataFileName']\n",
					"Schema = Schema + TaskObjectJson['Source']['Instance']['SourceRelativePath'] + \"/\" + TaskObjectJson['Source']['SchemaFileName']\n",
					"Target = Target + TaskObjectJson['Target']['Instance']['TargetRelativePath'] + \"/\" + TaskObjectJson['Target']['DataFileName']\n",
					"\n",
					"\n",
					"#remove any double slashes\n",
					"Source = Source.replace('//', '/')\n",
					"Schema = Schema.replace('//', '/')\n",
					"Target = Target.replace('//', '/')\n",
					"\n",
					"#get source and target data types\n",
					"SourceDT = TaskObjectJson['Source']['Type']\n",
					"TargetDT = TaskObjectJson['Target']['Type']\n",
					"\n",
					"\n",
					"#add abfss\n",
					"Source = \"abfss://\" + Source\n",
					"Schema = \"abfss://\" + Schema\n",
					"Target = \"abfss://\" + Target\n",
					"\n",
					"\n",
					"print (\"Source: \" + Source)\n",
					"print (\"Schema: \" + Schema)\n",
					"print (\"Target: \" + Target)\n",
					"\n"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#get scheme file ->  this needs to be made dynamic\n",
					"try:\n",
					"    schema = spark.read.load(Schema, format='json', multiLine=True)\n",
					"    #convert it into a list so we can loop it using python rules\n",
					"    schema = schema.collect()\n",
					"    #loop through each column to find the primary key column\n",
					"    #note -> we need to change this so that if a schema is not provided it will use the dataframe to generate one / assume the first column is the PK\n",
					"    for col in schema:\n",
					"        if col.PKEY_COLUMN:\n",
					"            print(col.COLUMN_NAME)\n",
					"            primaryKey = col\n",
					"            break\n",
					"    #set up the merge condition used in the next code block\n",
					"    mergeCondition = \"oldData.\" + primaryKey.COLUMN_NAME + \" = newData.\" + primaryKey.COLUMN_NAME\n",
					"except:\n",
					"    print(\"Schema json not found - assuming source dataframe first column is PK - WIP\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from delta.tables import *\n",
					"import pandas as pd\n",
					"if(TaskObjectJson['TMOptionals']['CDCSource'] == 'Enabled'):\n",
					"    print(\"CDC Source\")\n",
					"    df = spark.read.load(Source, format=SourceDT)\n",
					"    #these are our cdc specific columns\n",
					"    cdcCols = [\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$update_mask\", \"__$operation\", \"__$command_id\"]\n",
					"    #we are dropping all of the $ / _ as they cause issues with spark SQL functions -> this may be changed to just remove the first 3 chars of each of the cdcCols (__$)\n",
					"    cdcColsToDrop = [\"startlsn\", \"endlsn\", \"seqval\", \"updatemask\", \"operation\", \"commandid\"]\n",
					"    #these are columns we want to convert from binary data types to string as dataframes do not play nice with them currently\n",
					"   #colsToString = [\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$update_mask\"]\n",
					"    colsToString = [\"startlsn\", \"endlsn\", \"seqval\", \"updatemask\"]\n",
					"\n",
					"    for col in cdcCols:\n",
					"        new_col = col.replace('_','')\n",
					"        new_col = new_col.replace('$','')\n",
					"        df = df.withColumnRenamed(col, new_col)\n",
					"\n",
					"    for col in colsToString: \n",
					"        try:\n",
					"            df = df.withColumn(col, hex(col))\n",
					"        except:\n",
					"            print(\"Error converting the column \" + col)\n",
					"\n",
					"    #convert to pandas dataframe so we can do more manipulation\n",
					"    pdf = df.toPandas()\n",
					"\n",
					"    #we want to sort by our start lsn and then by the seqval so that we can drop everything except the most recent database change for every unique row\n",
					"    try:\n",
					"        #columns we are sorting by, the LSN and then the sequence value - ensure the latest is at the bottom of the table\n",
					"        pdf = pdf.sort_values(by=[\"startlsn\", \"seqval\"])\n",
					"    except:\n",
					"        print(\"error in finding valid sorting columns - skipping.\")\n",
					"    pdf_dedupe = pdf.drop_duplicates(subset=[primaryKey.COLUMN_NAME], keep='last', inplace=False)\n",
					"\n",
					"    df = spark.createDataFrame(pdf_dedupe)\n",
					"\n",
					"    #operation 1 is equal to delete, the other 3 operations (inserts old and new / upserts) can be done together\n",
					"    dfDeletes = df.filter(\"operation == 1\")\n",
					"    dfUpserts = df.filter(\"operation != 1\")\n",
					"    #We want to sort our columns by our primary key now that we have only the latest actions\n",
					"    dfDeletes = dfDeletes.sort(primaryKey.COLUMN_NAME)\n",
					"    dfUpserts = dfUpserts.sort(primaryKey.COLUMN_NAME)\n",
					"\n",
					"    #drop unwanted columns -> not needed for our delta table as they are cdc specific\n",
					"    for col in cdcColsToDrop:\n",
					"        try:\n",
					"            dfDeletes = dfDeletes.drop(col)\n",
					"            dfUpserts = dfUpserts.drop(col)\n",
					"        except:\n",
					"            print(\"Error dropping the column \" + col)\n",
					"\n",
					"    sql = 'describe detail \"' + Target + '\"'\n",
					"    try:\n",
					"        if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
					"            print(\"Table already exists. Performing Merge\")\n",
					"            olddt = DeltaTable.forPath(spark, Target)  \n",
					"            olddt.alias(\"oldData\").merge(\n",
					"                dfUpserts.alias(\"newData\"),\n",
					"                mergeCondition) \\\n",
					"            .whenMatchedUpdateAll() \\\n",
					"            .whenNotMatchedInsertAll() \\\n",
					"            .execute()\n",
					"        else:\n",
					"            print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
					"            dfUpserts.write.format(\"delta\").save(Target)\n",
					"    except: \n",
					"        print(\"Table does not exist, error thrown. Creating new Delta Table. Note - this error can be that no file is found.\") \n",
					"        dfUpserts.write.format(\"delta\").save(Target)\n",
					"\n",
					"    olddt = DeltaTable.forPath(spark, Target)  \n",
					"    olddt.alias(\"oldData\").merge(\n",
					"    dfDeletes.alias(\"newData\"),\n",
					"    mergeCondition) \\\n",
					"    .whenMatchedDelete() \\\n",
					"    .execute()  \n",
					"\n",
					"else:\n",
					"    print(\"Non CDC Source\")\n",
					"    if(TargetDT == 'Delta'):\n",
					"        print(\"SourceDT = \" + SourceDT + \", TargetDT = Delta.\")\n",
					"        df = spark.read.load(Source, format=SourceDT)\n",
					"        sql = 'describe detail \"' + Target + '\"'\n",
					"        try:\n",
					"            if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
					"                print(\"Table already exists. Performing Merge\")\n",
					"                olddt = DeltaTable.forPath(spark, Target)  \n",
					"                olddt.alias(\"oldData\").merge(\n",
					"                    df.alias(\"newData\"),\n",
					"                    mergeCondition) \\\n",
					"                .whenMatchedUpdateAll() \\\n",
					"                .whenNotMatchedInsertAll() \\\n",
					"                .execute()\n",
					"            else:\n",
					"                print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
					"                df.write.format(\"delta\").save(Target)\n",
					"        except:\n",
					"            print(\"Table does not exist. Creating new Delta Table.\")    \n",
					"            df.write.format(\"delta\").save(Target)\n",
					"    elif(TargetDT == 'Parquet' and SourceDT == 'Delta'):\n",
					"        print(\"SourceDT = Delta, TargetDT = Parquet.\")\n",
					"        df = spark.read.format(\"delta\").load(Source)\n",
					"        df.write.format(\"parquet\").mode(\"overwrite\").save(Target) "
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#This checks if the user wants to save the sink as a persistent table\n",
					"if(TaskObjectJson['TMOptionals']['SparkTableCreate'] == 'Enabled'):\n",
					"    print(\"Creating Spark Table\")\n",
					"    df = spark.read.load(Target, format='delta')\n",
					"    #targetDB = 'testdb2'\n",
					"    #targetTable = 'test2'\n",
					"    #SynapseSnapshots/Workspacename/dbname/tablename.parquet\n",
					"    #SynapseTarget = Target + '/'+ targetDB + '/' + targetTable\n",
					"    #SynapseTarget = 'abfss://datalakeraw@' + TaskObjectJson['TMOptionals']['PersistentStorage']\n",
					"    targetDB = TaskObjectJson['TMOptionals']['SparkTableDBName']\n",
					"    targetTable = TaskObjectJson['TMOptionals']['SparkTableName']\n",
					"    #if the target datatype is parquet then we do not need to create a copy of the data - we can use the recently saved sink target\n",
					"    if (TargetDT == 'Parquet'):\n",
					"        SnapshotTarget = Target\n",
					"    else:\n",
					"        SnapshotTarget = Target + '/'+ TaskObjectJson['TMOptionals']['SparkTableDBName'] + '/' + TaskObjectJson['TMOptionals']['SparkTableName']\n",
					"        #we need to update the parquet file - this is not very efficient but there isnt a current better way as delta tables are not supported for persistent tables\n",
					"        df.write.format(\"parquet\").mode(\"overwrite\").save(SnapshotTarget)\n",
					"\n",
					"\n",
					"    #we need to make the DB and table lowercase as synapse persistent tables dont identify them as different identities\n",
					"    targetDB = targetDB.lower()\n",
					"    targetTable = targetTable.lower()\n",
					"\n",
					"    #check if the specified DB / table exists - if so only do required actions.\n",
					"    dbList = spark.catalog.listDatabases()\n",
					"    dbExists = False\n",
					"    for db in dbList:\n",
					"        if (db.name == targetDB):\n",
					"            dbExists = True\n",
					"            break\n",
					"    if (dbExists):\n",
					"        print(\"DB Exists\")\n",
					"        tableExists = False\n",
					"        spark.catalog.setCurrentDatabase(targetDB)\n",
					"        tableList = spark.catalog.listTables()\n",
					"        for table in tableList:\n",
					"            if (table.name == targetTable):\n",
					"                tableExists = True\n",
					"                break\n",
					"        if (tableExists):\n",
					"            print(\"Table exists - nothing needed to be done\")\n",
					"            spark.catalog.refreshTable(targetTable)\n",
					"        else:\n",
					"            print(\"Table doesnt exist - creating\")\n",
					"            spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\n",
					"    else:\n",
					"        print(\"DB Doesnt exist - creating DB and table\")\n",
					"        createDBString = \"CREATE DATABASE \" + targetDB \n",
					"        spark.sql(createDBString)\n",
					"        spark.catalog.setCurrentDatabase(targetDB)\n",
					"        spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\n",
					"else:\n",
					"    print(\"Skipping Spark Table creation\")\n",
					"\n",
					"#%%sql\n",
					"#CREATE TABLE testdb.dbo.test\n",
					"#USING PARQUET OPTIONS ('path'= 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT.Customer.chunk_2.parquet', 'inferschema'=true);\n",
					"#select * from testdb.dbo.test limit 10\n",
					"\n",
					"    "
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#olddt.history().show(20, 1000, False)\n",
					"#display(spark.read.format(\"delta\").load(Target))\n",
					"#spark.sql(\"CREATE TABLE SalesLTCustomer USING DELTA LOCATION '{0}'\".format(TargetFile))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Execute Upsert\n",
					"#(old_deltaTable\n",
					"# .alias(\"oldData\") \n",
					"# .merge(newIncrementalData.alias(\"newData\"), \"oldData.id = newData.id\")\n",
					"# .whenMatchedUpdate(set = {\"name\": col(\"newData.name\")})\n",
					"# .whenNotMatchedInsert(values = {\"id\": col(\"newData.id\"), \"name\":\n",
					"#                                col(\"newData.name\")})\n",
					"# .execute()\n",
					"#)\n",
					"#\n",
					"# Display the records to check if the records are Merged\n",
					"#display(spark.read.format(\"delta\").load(Target))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#olddt.history().show(20, 1000, False)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#########################\n",
					"#NOTE -> This is an alternate way of upserting into delta table. Using manual method of getting each column required from the schema / dataframe and then creating a dictionary to use for the upsert.\n",
					"#           Currently not using this however it does work. Would advise to change the script to just create a dictionary and insert that instead of creating a string to convert into a dict.\n",
					"#########################\n",
					"#from delta.tables import *\n",
					"#df = spark.read.load(Source, format='parquet')\n",
					"#updatecols = []\n",
					"#insertcols = []\n",
					"#for col in schema:\n",
					"#    updatecols.append(col.COLUMN_NAME)\n",
					"#\n",
					"#for col in df.dtypes:\n",
					"#    insertcols.append(col[0])\n",
					"#\n",
					"#creating a string to be converted to dictionary \n",
					"#note -> can easily re-write this as just a dictionary if end up using this method.\n",
					"#updatestring = '{'\n",
					"#insertstring = '{'\n",
					"#\n",
					"#Go through each column in the schema to check what needs to be updated\n",
					"#for col in updatecols:\n",
					"#    updatestring = updatestring + '\"' + col + '\": \"newData.' + col +'\", '\n",
					"#updatestring = updatestring[:-2]\n",
					"#updatestring = updatestring + '}'\n",
					"#\n",
					"#Go through the new data to check what columns need to be inserted\n",
					"#for col in insertcols:\n",
					"#    insertstring = insertstring + '\"' + col + '\": \"newData.' + col +'\", '\n",
					"#insertstring = insertstring[:-2]\n",
					"#insertstring = insertstring + '}'\n",
					"#\n",
					"#print(updatestring)\n",
					"#print(insertstring)\n",
					"#\n",
					"#convert to dict\n",
					"#res = json.loads(updatestring)\n",
					"#res2 = json.loads(insertstring)\n",
					"#\n",
					"#sql = 'describe detail \"' + Target + '\"'\n",
					"#try:\n",
					"#    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
					"#        print(\"Table already exists. Performing Merge\")\n",
					"#        olddt = DeltaTable.forPath(spark, Target)  \n",
					"#        olddt.alias(\"oldData\").merge(\n",
					"#            df.alias(\"newData\"),\n",
					"#            mergeCondition) \\\n",
					"#        .whenMatchedUpdate(set = res) \\\n",
					"#        .whenNotMatchedInsert(values = res2) \\\n",
					"#        .execute()\n",
					"#except:\n",
					"#    print(\"Table does not exist.\")    \n",
					"#    df.write.format(\"delta\").save(Target)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#from delta.tables import * \n",
					"#import pandas as pd\n",
					"#Source = 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT.Customer.chunk_1.parquet'\n",
					"#Target = 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT_Customer_Delta/SalesLT.Customer'\n",
					"#mergeCondition = \"oldData.\" + \"CustomerID\" + \" = newData.\" + \"CustomerID\"\n",
					"\n",
					"#df = spark.read.load(Source, format='parquet')\n",
					"\n",
					"#these are our cdc specific columns\n",
					"#cdcCols = ['__$start_lsn', '__$end_lsn', '__$seqval', '__$operation', '__$update_mask', '__$command_id']\n",
					"\n",
					"#convert to pandas dataframe so we can do more manipulation\n",
					"#pdf = df.toPandas()\n",
					"#we want to sort by our start lsn and then by the seqval so that we can drop everything except the most recent database change for every unique row\n",
					"#try:\n",
					"    #columns we are sorting by, the LSN and then the sequence value - ensure the latest is at the bottom of the table\n",
					"#    pdf = pdf.sort_values(by=['__$start_lsn', '__$seqval'])\n",
					"#except:\n",
					"#   print(\"error in finding valid sorting columns - skipping.\")\n",
					"\n",
					"#df = pdf.drop_duplicates(subset=['CustomerID'], keep='last', inplace=False)\n",
					"#df_dedupe = pdf.drop_duplicates(subset=[primaryKey.COLUMN_NAME], keep='last', inplace=False)\n",
					"\n",
					"#df = spark.createDataFrame(df)\n",
					"#dfDeletes = df.filter(\"CustomerID < 100\")\n",
					"#dfUpserts = df.filter(\"CustomerID >= 100\")\n",
					"#dfDeletes = df.filter(\"__$operation == 1\")\n",
					"#dfUpserts = df.filter(\"__$operation != 1\")\n",
					"\n",
					"#drop unwanted columns -> not needed for our delta table as they are cdc specific\n",
					"#for col in cdcCols:\n",
					"#    try:\n",
					"#        dfDeletes = dfDeletes.drop(col)\n",
					"#        dfUpserts = dfUpserts.drop(col)\n",
					"#    except:\n",
					"#        print(\"Error dropping the column \" + col)\n",
					"\n",
					"#sql = 'describe detail \"' + Target + '\"'\n",
					"#try:\n",
					"#    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
					"#        print(\"Table already exists. Performing Merge\")\n",
					"#        olddt = DeltaTable.forPath(spark, Target)  \n",
					"#        olddt.alias(\"oldData\").merge(\n",
					"#            dfUpserts.alias(\"newData\"),\n",
					"#           mergeCondition) \\\n",
					"#        .whenMatchedUpdateAll() \\\n",
					"#        .whenNotMatchedInsertAll() \\\n",
					"#        .execute()\n",
					"#    else:\n",
					"#        print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
					"#       dfUpserts.write.format(\"delta\").save(Target)\n",
					"#except: \n",
					"#    print(\"Table does not exist, error thrown. Creating new Delta Table.\")    \n",
					"#    dfUpserts.write.format(\"delta\").save(Target)\n",
					"\n",
					"#olddt = DeltaTable.forPath(spark, Target)  \n",
					"#olddt.alias(\"oldData\").merge(\n",
					"#dfDeletes.alias(\"newData\"),\n",
					"#mergeCondition) \\\n",
					"#.whenMatchedDelete() \\\n",
					"#.execute()  \n",
					"#display(dfDeletes)\n",
					"\n",
					"#df.write.format(\"delta\").save(Target)\n"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#from delta.tables import * \n",
					"#import pandas as pd\n",
					"\n",
					"#df = spark.createDataFrame([\"0x0000019600000178002D\",\"0x0000019600000178002D\",\"0x0000019600000178002D\", \"0x0000019600000178002A\"], \"string\").toDF(\"hex\")\n",
					"#hex2 = ['0x00000194000000A80002', '0x00000A94000000A80002', '0x00000194000000B80004', '0x00000194000000B80000']\n",
					"\n",
					"#pdf = df.toPandas()\n",
					"#pdf['hex2'] = hex2\n",
					"\n",
					"\n",
					"#df = spark.createDataFrame(pdf)\n",
					"#df_dedupe = df.dropDuplicates('hex')\n",
					"#df_dedupe = df.dropDuplicates(primaryKey.COLUMN_NAME)\n",
					"\n",
					"\n",
					"#pdf['hex'] = pdf['hex'].apply(int, base=16)\n",
					"\n",
					"#pdf = pdf.sort_values(by=['hex','hex2'])\n",
					"#show(df_dedupe)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [],
				"attachments": null,
				"execution_count": null
			}
		]
	}
}